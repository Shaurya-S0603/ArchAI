{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "collapsed_sections": [
        "Iyql1szfIpxd",
        "M2rPqy4M2eMV",
        "spFp6rSYKMZ2"
      ],
      "mount_file_id": "1NfjmX-nb4GszvWmmGuyPpESpYTHFRBVQ",
      "authorship_tag": "ABX9TyPtYpLeSmca72JMkJ3nTLzL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shaurya-S0603/ArchAI/blob/main/ArchAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAINING (GPU)**"
      ],
      "metadata": {
        "id": "Iyql1szfIpxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import gc\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import DatasetFolder\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"✅ Using device: {device}\")\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "IMG_SIZE = 64\n",
        "LATENT_DIM = 100\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 0.0002\n",
        "ACCUM_STEPS = 2\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transform):\n",
        "        self.images = [os.path.join(root, f) for f in os.listdir(root) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.images[idx]).convert(\"RGB\")\n",
        "        return self.transform(image), 0\n",
        "\n",
        "dataset = ImageDataset(\"/content/drive/MyDrive/ArchAI_Dataset/train\", transform)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.init_size = IMG_SIZE // 4\n",
        "        self.l1 = nn.Sequential(nn.Linear(LATENT_DIM, 128 * self.init_size ** 2))\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128, 0.8),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64, 0.8),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 3, 3, stride=1, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = self.l1(z)\n",
        "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
        "        img = self.conv_blocks(out)\n",
        "        return img\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * (IMG_SIZE // 4) * (IMG_SIZE // 4), 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.model(img)\n",
        "\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "checkpoint_dir = \"/content/drive/MyDrive/ArchAI/models\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "generator_path = f\"{checkpoint_dir}/generator.pth\"\n",
        "discriminator_path = f\"{checkpoint_dir}/discriminator.pth\"\n",
        "\n",
        "if os.path.exists(generator_path) and os.path.exists(discriminator_path):\n",
        "    generator.load_state_dict(torch.load(generator_path, map_location=device))\n",
        "    discriminator.load_state_dict(torch.load(discriminator_path, map_location=device))\n",
        "    print(\"✅ Loaded latest checkpoint\")\n",
        "\n",
        "generator = torch.compile(generator)\n",
        "discriminator = torch.compile(discriminator)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "scaler = torch.amp.GradScaler(\"cuda\")\n",
        "\n",
        "def train_gan(epochs=EPOCHS):\n",
        "    for epoch in range(epochs):\n",
        "        for i, (real_images, _) in enumerate(dataloader):\n",
        "            real_images = real_images.to(device)\n",
        "            batch_size = real_images.size(0)\n",
        "            real_labels = torch.ones(batch_size, 1).to(device)\n",
        "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "            optimizer_G.zero_grad()\n",
        "            z = torch.randn(batch_size, LATENT_DIM).to(device)\n",
        "            with torch.amp.autocast(\"cuda\"):\n",
        "                fake_images = generator(z)\n",
        "                fake_output = discriminator(fake_images)\n",
        "                loss_G = criterion(fake_output, real_labels) / ACCUM_STEPS\n",
        "            scaler.scale(loss_G).backward()\n",
        "            if (i + 1) % ACCUM_STEPS == 0:\n",
        "                scaler.step(optimizer_G)\n",
        "                scaler.update()\n",
        "                optimizer_G.zero_grad()\n",
        "\n",
        "            optimizer_D.zero_grad()\n",
        "            with torch.amp.autocast(\"cuda\"):\n",
        "                real_output = discriminator(real_images)\n",
        "                loss_real = criterion(real_output, real_labels)\n",
        "                fake_output = discriminator(fake_images.detach())\n",
        "                loss_fake = criterion(fake_output, fake_labels)\n",
        "                loss_D = (loss_real + loss_fake) / (2 * ACCUM_STEPS)\n",
        "            scaler.scale(loss_D).backward()\n",
        "            if (i + 1) % ACCUM_STEPS == 0:\n",
        "                scaler.step(optimizer_D)\n",
        "                scaler.update()\n",
        "                optimizer_D.zero_grad()\n",
        "\n",
        "            if i % 500 == 0:\n",
        "                print(f\"Epoch [{epoch}/{epochs}] | Batch [{i}/{len(dataloader)}] | Loss D: {loss_D.item():.4f}, Loss G: {loss_G.item():.4f}\")\n",
        "\n",
        "            del real_images, fake_images, fake_output, real_output, loss_G, loss_D, loss_real, loss_fake, z\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            torch.save(generator.state_dict(), f\"/content/drive/MyDrive/ArchAI/models/generator.pth\")\n",
        "            torch.save(discriminator.state_dict(), f\"/content/drive/MyDrive/ArchAI/models/discriminator.pth\")\n",
        "            print(f\"✅ Checkpoint saved at epoch {epoch}\")\n",
        "\n",
        "train_gan(epochs=EPOCHS)\n",
        "\n",
        "torch.save(generator.state_dict(), \"/content/drive/MyDrive/ArchAI/models/generator.pth\")\n",
        "torch.save(discriminator.state_dict(), \"/content/drive/MyDrive/ArchAI/models/discriminator.pth\")\n",
        "print(\"✅ Final models saved!\")\n"
      ],
      "metadata": {
        "id": "NEpgHMei60nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAINING (CPU)**"
      ],
      "metadata": {
        "id": "M2rPqy4M2eMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import gc\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import DatasetFolder\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "print(f\"✅ Using device: {device}\")\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "IMG_SIZE = 64\n",
        "LATENT_DIM = 100\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 0.0002\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transform):\n",
        "        self.images = [os.path.join(root, f) for f in os.listdir(root) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.images[idx]).convert(\"RGB\")\n",
        "        return self.transform(image), 0\n",
        "\n",
        "dataset = ImageDataset(\"/content/drive/MyDrive/ArchAI_Dataset/train\", transform)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.init_size = IMG_SIZE // 4\n",
        "        self.l1 = nn.Sequential(nn.Linear(LATENT_DIM, 128 * self.init_size ** 2))\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128, 0.8),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64, 0.8),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 3, 3, stride=1, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = self.l1(z)\n",
        "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
        "        img = self.conv_blocks(out)\n",
        "        return img\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * (IMG_SIZE // 4) * (IMG_SIZE // 4), 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.model(img)\n",
        "\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "checkpoint_dir = \"/content/drive/MyDrive/ArchAI/models\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "generator_path = f\"{checkpoint_dir}/generator_cpu.pth\"\n",
        "discriminator_path = f\"{checkpoint_dir}/discriminator_cpu.pth\"\n",
        "\n",
        "if os.path.exists(generator_path) and os.path.exists(discriminator_path):\n",
        "    generator.load_state_dict(torch.load(generator_path, map_location=torch.device('cpu')))\n",
        "    discriminator.load_state_dict(torch.load(discriminator_path, map_location=torch.device('cpu')))\n",
        "    print(\"✅ Loaded latest checkpoint\")\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for i, (real_images, _) in enumerate(dataloader):\n",
        "        real_images = real_images.to(device)\n",
        "        batch_size = real_images.size(0)\n",
        "        real_labels = torch.ones(batch_size, 1).to(device)\n",
        "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        z = torch.randn(batch_size, LATENT_DIM).to(device)\n",
        "        fake_images = generator(z)\n",
        "        fake_output = discriminator(fake_images)\n",
        "        loss_G = criterion(fake_output, real_labels)\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        real_output = discriminator(real_images)\n",
        "        loss_real = criterion(real_output, real_labels)\n",
        "        fake_output = discriminator(fake_images.detach())\n",
        "        loss_fake = criterion(fake_output, fake_labels)\n",
        "        loss_D = (loss_real + loss_fake) / 2\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        if i % 500 == 0:\n",
        "            print(f\"Epoch [{epoch}/{EPOCHS}] | Batch [{i}/{len(dataloader)}] | Loss D: {loss_D.item():.4f}, Loss G: {loss_G.item():.4f}\")\n",
        "\n",
        "        del real_images, fake_images, fake_output, real_output, loss_G, loss_D, loss_real, loss_fake, z\n",
        "        gc.collect()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        torch.save(generator.state_dict(), f\"/content/drive/MyDrive/ArchAI/models/generator_cpu.pth\")\n",
        "        torch.save(discriminator.state_dict(), f\"/content/drive/MyDrive/ArchAI/models/discriminator_cpu.pth\")\n",
        "        print(f\"✅ Checkpoint saved at epoch {epoch}\")\n",
        "\n",
        "torch.save(generator.state_dict(), \"/content/drive/MyDrive/ArchAI/models/generator_cpu.pth\")\n",
        "torch.save(discriminator.state_dict(), \"/content/drive/MyDrive/ArchAI/models/discriminator_cpu.pth\")\n",
        "print(\"✅ Final models saved!\")\n"
      ],
      "metadata": {
        "id": "nHK2wv-x2jMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TESTING**"
      ],
      "metadata": {
        "id": "spFp6rSYKMZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"✅ Using device: {device}\")\n",
        "\n",
        "generator = Generator().to(device)\n",
        "\n",
        "load = int(input(\"Trained on CPU or GPU? (1,2): \"))\n",
        "if load == 1:\n",
        "  generator.load_state_dict(torch.load(\"/content/drive/MyDrive/ArchAI/models/generator_cpu.pth\", map_location=torch.device('cpu')))\n",
        "else:\n",
        "  generator.load_state_dict(torch.load(\"/content/drive/MyDrive/ArchAI/models/generator.pth\"))\n",
        "generator.eval()\n",
        "\n",
        "def generate_images(num_images=5):\n",
        "    z = torch.randn(num_images, LATENT_DIM).to(device)\n",
        "    with torch.no_grad():\n",
        "        fake_images = generator(z).cpu()\n",
        "\n",
        "    fake_images = (fake_images + 1) / 2\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(15, 15))\n",
        "    for i in range(num_images):\n",
        "        img = transforms.ToPILImage()(fake_images[i])\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].axis(\"off\")\n",
        "    plt.show()\n",
        "    print()\n",
        "\n",
        "generate_images()\n"
      ],
      "metadata": {
        "id": "hufxSZHxGaVS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}